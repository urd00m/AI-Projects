{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This file processes the provided data `tweets.csv`. It is downloaded from https://dataverse.harvard.edu/dataset.xhtml?id=3047332."
   ],
   "metadata": {
    "id": "UO_e4YzCIIY6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "outputs": [],
   "metadata": {
    "id": "xiPnFLl_HFQ1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tweets_df = pd.read_csv(\"data/tweets.csv\")\n",
    "tweets_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>country</th>\n",
       "      <th>date_time</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number_of_likes</th>\n",
       "      <th>number_of_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/01/2017 19:52</td>\n",
       "      <td>8.196330e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7900</td>\n",
       "      <td>3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 08:38</td>\n",
       "      <td>8.191010e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3689</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:52</td>\n",
       "      <td>8.190140e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10341</td>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:44</td>\n",
       "      <td>8.190120e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10774</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/01/2017 05:22</td>\n",
       "      <td>8.186890e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17620</td>\n",
       "      <td>4655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52537</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Life couldn't be better right now. üòä</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2015 23:10</td>\n",
       "      <td>5.526030e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32799</td>\n",
       "      <td>23796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52538</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>First Monday back in action. I'd say 21.6 mile...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2015 02:17</td>\n",
       "      <td>5.522880e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21709</td>\n",
       "      <td>12511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52539</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Crime shows, buddy, snuggles = the perfect Sun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 03:42</td>\n",
       "      <td>5.519470e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25269</td>\n",
       "      <td>15583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52540</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>‚ùÑÔ∏è http://t.co/sHCFdPpGPa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 00:06</td>\n",
       "      <td>5.518920e+17</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15985</td>\n",
       "      <td>10456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52541</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>‚ù§Ô∏è‚ùÑÔ∏è‚úàÔ∏è http://t.co/ixmB5lv17Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 00:02</td>\n",
       "      <td>5.518910e+17</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16193</td>\n",
       "      <td>10822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52542 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                            content country  \\\n",
       "0      katyperry  Is history repeating itself...?#DONTNORMALIZEH...     NaN   \n",
       "1      katyperry  @barackobama Thank you for your incredible gra...     NaN   \n",
       "2      katyperry                Life goals. https://t.co/XIn1qKMKQl     NaN   \n",
       "3      katyperry            Me right now üôèüèª https://t.co/gW55C1wrwd     NaN   \n",
       "4      katyperry  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...     NaN   \n",
       "...          ...                                                ...     ...   \n",
       "52537   ddlovato               Life couldn't be better right now. üòä     NaN   \n",
       "52538   ddlovato  First Monday back in action. I'd say 21.6 mile...     NaN   \n",
       "52539   ddlovato  Crime shows, buddy, snuggles = the perfect Sun...     NaN   \n",
       "52540   ddlovato                          ‚ùÑÔ∏è http://t.co/sHCFdPpGPa     NaN   \n",
       "52541   ddlovato                      ‚ù§Ô∏è‚ùÑÔ∏è‚úàÔ∏è http://t.co/ixmB5lv17Z     NaN   \n",
       "\n",
       "              date_time            id language  latitude  longitude  \\\n",
       "0      12/01/2017 19:52  8.196330e+17       en       NaN        NaN   \n",
       "1      11/01/2017 08:38  8.191010e+17       en       NaN        NaN   \n",
       "2      11/01/2017 02:52  8.190140e+17       en       NaN        NaN   \n",
       "3      11/01/2017 02:44  8.190120e+17       en       NaN        NaN   \n",
       "4      10/01/2017 05:22  8.186890e+17       en       NaN        NaN   \n",
       "...                 ...           ...      ...       ...        ...   \n",
       "52537  06/01/2015 23:10  5.526030e+17       en       NaN        NaN   \n",
       "52538  06/01/2015 02:17  5.522880e+17       en       NaN        NaN   \n",
       "52539  05/01/2015 03:42  5.519470e+17       en       NaN        NaN   \n",
       "52540  05/01/2015 00:06  5.518920e+17      und       NaN        NaN   \n",
       "52541  05/01/2015 00:02  5.518910e+17      und       NaN        NaN   \n",
       "\n",
       "       number_of_likes  number_of_shares  \n",
       "0                 7900              3472  \n",
       "1                 3689              1380  \n",
       "2                10341              2387  \n",
       "3                10774              2458  \n",
       "4                17620              4655  \n",
       "...                ...               ...  \n",
       "52537            32799             23796  \n",
       "52538            21709             12511  \n",
       "52539            25269             15583  \n",
       "52540            15985             10456  \n",
       "52541            16193             10822  \n",
       "\n",
       "[52542 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "id": "8Ua_dRmRB0yk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {
    "id": "tr6k3V66CMoB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tweets = list(tweets_df['content'][:100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "inputs = tokenizer(tweets, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "labels = torch.tensor(len(tweets_df['author'][:100])).unsqueeze(0)"
   ],
   "outputs": [],
   "metadata": {
    "id": "oydhAntrCOp9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "# input_ids = torch.tensor([tokenizer.encode(tweets[0])])"
   ],
   "outputs": [],
   "metadata": {
    "id": "EaNr0kGpHCJD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "with torch.no_grad():\n",
    "    features = bertweet(inputs.input_ids)  # Models outputs are now tuples"
   ],
   "outputs": [],
   "metadata": {
    "id": "Xtkg8GoVItm0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "features.pooler_output.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {
    "id": "A8-8vjgmMEo2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataset and DataLoader"
   ],
   "metadata": {
    "id": "VHDni4DiS1iT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, size=100):\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "        self.inputs = tokenizer(list(df['content'][:size]), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        self.authors = pd.get_dummies(df['author']).loc[:size].values\n",
    "        \n",
    "        # Run BERT forward pass to get embeddings\n",
    "        with torch.no_grad():\n",
    "            features = bertweet(self.inputs.input_ids)\n",
    "            \n",
    "        self.embeddings = features.pooler_output\n",
    "        self.labels = torch.tensor(self.authors).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.embeddings[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "ds = TweetDataset(tweets_df, size=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "ds[0][0].shape[0] == 768\n",
    "ds[0][1].shape[0] == 20"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-15481da3cb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bootcamp Day 2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}